# -*- coding: utf-8 -*-
"""GPT4o_Lanchain_RAG (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UJZtWu0SZyflvcZGjIsLZUjl9gcHtQs_
"""

## RAG Application With GPT-4o

!pip install langchain faiss-cpu huggingface-hub langchain_community pypdf transformers sentence-transformers

"""# 1. Get a Data Loader

"""

from langchain.document_loaders import PyPDFLoader

# pdf_path = "/Users/rasagnyagudipudi/Desktop/chatbot/input.pdf"

pdf_path = "input.pdf"

loader = PyPDFLoader(pdf_path)

pages = loader.load_and_split()

pages

"""# 2. Convert data to Vector Database

"""

# from langchain_objectbox.vectorstores import ObjectBox ##vector Database
# from langchain_openai import OpenAIEmbeddings

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1024,
    chunk_overlap=64,
    separators=['\n\n', '\n', '(?=>\. )', ' ', '']
)
docs  = text_splitter.split_documents(pages)

docs

from langchain.embeddings import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings()
embeddings

"""# 3. Make a RAG pipeline

"""

from langchain.vectorstores import FAISS
db = FAISS.from_documents(docs, embeddings)

from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map="auto")

hf_pipeline = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=500,
    do_sample=True,
    temperature=0.7
)

llm = HuggingFacePipeline(pipeline=hf_pipeline)

from langchain.chains.question_answering import load_qa_chain

retriever = db.as_retriever(search_kwargs={"k": 5})
chain = load_qa_chain(llm, chain_type="stuff")

q = "what is the Eligibility Criteria for registering for Placement Activities?"

retrieved_docs = retriever.get_relevant_documents(q)

context = " ".join([doc.page_content for doc in retrieved_docs])

query = f"""
Question: {q}

If the below context is relevant to the question, provide a detailed answer. If the context is irrelevant, respond with exactly:
'Sorry, I didn't understand your question. Do you want to connect with a live agent?'

Context:
{context}


"""
full_prompt = query

response = chain.invoke({
    "input_documents":retrieved_docs,
    "question":full_prompt})
user_readable_answer = response["output_text"]
user_readable_answer

response

